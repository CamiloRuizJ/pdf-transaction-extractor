"""
RExeli API - 25MB File Upload Support
Vercel-compatible serverless function
"""

import os
import json
import tempfile
import logging
import uuid
from datetime import datetime, timedelta
from typing import Dict, Any, Tuple, List
import base64
import io

from flask import Flask, jsonify, request
from flask_cors import CORS
from werkzeug.utils import secure_filename
from werkzeug.exceptions import RequestEntityTooLarge

# Load environment variables
try:
    from dotenv import load_dotenv
    load_dotenv()  # Load .env file
except ImportError:
    print("python-dotenv not available - using system environment variables only")

# AI/ML imports
try:
    import openai
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("OpenAI not available - install with 'pip install openai'")

# Import Supabase client
try:
    from supabase import create_client, Client
    SUPABASE_AVAILABLE = True
except ImportError:
    SUPABASE_AVAILABLE = False

# Configuration
MAX_CONTENT_LENGTH = int(os.environ.get('MAX_CONTENT_LENGTH', 26214400))  # 25MB
ALLOWED_EXTENSIONS = {'pdf', 'doc', 'docx', 'xls', 'xlsx', 'jpg', 'png'}
UPLOAD_FOLDER = '/tmp/uploads'

# Supabase Configuration
SUPABASE_URL = os.environ.get('SUPABASE_URL')
SUPABASE_ANON_KEY = os.environ.get('SUPABASE_ANON_KEY')
SUPABASE_SERVICE_KEY = os.environ.get('SUPABASE_SERVICE_KEY')
SUPABASE_BUCKET = 'documents'

# OpenAI Configuration
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
OPENAI_MODEL = os.environ.get('OPENAI_MODEL', 'gpt-4o-mini')

# File size thresholds
DIRECT_UPLOAD_THRESHOLD = 4 * 1024 * 1024  # 4MB - files above this use direct Supabase upload
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB - absolute maximum (Supabase Free tier limit)

# Create Flask app
app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH
app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY', 'dev-key')

# Setup CORS
CORS(app, resources={"/*": {"origins": "*"}})

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ensure upload directory exists
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Initialize Supabase client
supabase_client = None
if SUPABASE_AVAILABLE and SUPABASE_URL and SUPABASE_ANON_KEY:
    try:
        # Use anon key for now (service key can be added later for admin operations)
        supabase_client = create_client(SUPABASE_URL, SUPABASE_ANON_KEY)
        logger.info("Supabase client initialized successfully with anon key")
    except Exception as e:
        logger.error(f"Failed to initialize Supabase client: {str(e)}")
        supabase_client = None

# Initialize OpenAI client
openai_client = None
logger.info(f"OPENAI_AVAILABLE: {OPENAI_AVAILABLE}")
logger.info(f"OPENAI_API_KEY present: {bool(OPENAI_API_KEY)}")
logger.info(f"SUPABASE_AVAILABLE: {SUPABASE_AVAILABLE}")
logger.info(f"SUPABASE_URL present: {bool(SUPABASE_URL)}")
logger.info(f"SUPABASE_ANON_KEY present: {bool(SUPABASE_ANON_KEY)}")
logger.info(f"Supabase client initialized: {supabase_client is not None}")
logger.info(f"OPENAI_API_KEY length: {len(OPENAI_API_KEY) if OPENAI_API_KEY else 0}")

if OPENAI_AVAILABLE and OPENAI_API_KEY:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
        logger.info("OpenAI client initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize OpenAI client: {str(e)}")
        openai_client = None
else:
    if not OPENAI_AVAILABLE:
        logger.warning("OpenAI package not available")
    if not OPENAI_API_KEY:
        logger.warning("OPENAI_API_KEY environment variable not found")

def handle_error(message: str, status_code: int = 500) -> Tuple[Dict[str, Any], int]:
    """Simple error handling"""
    logger.error(f"API Error {status_code}: {message}")
    return {
        'success': False,
        'error': message,
        'timestamp': datetime.utcnow().isoformat()
    }, status_code

def allowed_file(filename: str) -> bool:
    """Check if file extension is allowed"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def generate_file_path(filename: str) -> str:
    """Generate a unique file path for storage"""
    file_id = str(uuid.uuid4())
    timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
    extension = os.path.splitext(filename)[1].lower()
    return f"uploads/{timestamp}_{file_id}{extension}"

def create_signed_upload_url(filename: str, file_size: int, content_type: str) -> Dict[str, Any]:
    """Create a signed URL for direct upload to Supabase"""
    if not supabase_client:
        return {'success': False, 'error': 'Supabase not configured'}
    
    try:
        # Generate unique file path
        file_path = generate_file_path(filename)
        
        # Create signed upload URL (expires in 1 hour)
        expires_in = 3600
        
        # Note: Supabase doesn't have direct presigned POST URLs like S3
        # We'll use a different approach - return upload info for direct client upload
        upload_token = str(uuid.uuid4())
        
        return {
            'success': True,
            'upload_method': 'direct_supabase',
            'file_path': file_path,
            'upload_token': upload_token,
            'supabase_url': SUPABASE_URL,
            'bucket': SUPABASE_BUCKET,
            'anon_key': SUPABASE_ANON_KEY,  # Safe to expose anon key
            'expires_in': expires_in,
            'max_file_size': MAX_FILE_SIZE,
            'instructions': {
                'method': 'POST',
                'headers': {
                    'Authorization': f'Bearer {SUPABASE_ANON_KEY}',
                    'Content-Type': content_type
                }
            }
        }
    except Exception as e:
        logger.error(f"Failed to create signed upload URL: {str(e)}")
        return {'success': False, 'error': str(e)}

# Health check endpoint
@app.route('/health', methods=['GET'])
@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.utcnow().isoformat(),
        'version': '3.0.0',
        'max_file_size_mb': MAX_FILE_SIZE // (1024*1024),
        'direct_upload_threshold_mb': DIRECT_UPLOAD_THRESHOLD // (1024*1024),
        'allowed_extensions': list(ALLOWED_EXTENSIONS),
        'supabase_available': supabase_client is not None,
        'openai_available': openai_client is not None,
        'ai_model': OPENAI_MODEL if openai_client else 'not configured',
        'upload_methods': ['server_upload', 'direct_supabase'] if supabase_client else ['server_upload']
    })

# Get upload method endpoint
@app.route('/upload-method', methods=['POST'])
@app.route('/api/upload-method', methods=['POST'])
def get_upload_method():
    """Determine upload method based on file size"""
    try:
        data = request.get_json()
        if not data:
            return handle_error("Request body required", 400)
        
        filename = data.get('filename')
        file_size = data.get('file_size', 0)
        content_type = data.get('content_type', 'application/pdf')
        
        if not filename:
            return handle_error("Filename is required", 400)
        
        if not allowed_file(filename):
            return handle_error("File type not allowed", 400)
        
        # Check file size limits
        if file_size > MAX_FILE_SIZE:
            max_mb = MAX_FILE_SIZE // (1024*1024)
            actual_mb = file_size / (1024*1024)
            return handle_error(f"File too large: {actual_mb:.1f}MB. Maximum allowed: {max_mb}MB", 413)
        
        # Determine upload method
        if file_size > DIRECT_UPLOAD_THRESHOLD and supabase_client:
            # Use direct Supabase upload for large files
            upload_info = create_signed_upload_url(filename, file_size, content_type)
            if not upload_info['success']:
                return handle_error(upload_info['error'], 500)
            
            return jsonify({
                'success': True,
                'upload_method': 'direct_supabase',
                'file_size_mb': file_size / (1024*1024),
                'threshold_mb': DIRECT_UPLOAD_THRESHOLD / (1024*1024),
                'upload_info': upload_info,
                'next_step': 'Upload directly to Supabase, then call /api/confirm-upload'
            })
        else:
            # Use server upload for smaller files
            return jsonify({
                'success': True,
                'upload_method': 'server_upload',
                'file_size_mb': file_size / (1024*1024),
                'threshold_mb': DIRECT_UPLOAD_THRESHOLD / (1024*1024),
                'upload_info': {
                    'endpoint': '/api/upload',
                    'method': 'POST',
                    'max_size_mb': MAX_CONTENT_LENGTH // (1024*1024)
                },
                'next_step': 'Upload to /api/upload endpoint'
            })
    
    except Exception as e:
        logger.error(f"Upload method determination failed: {str(e)}")
        return handle_error(f"Failed to determine upload method: {str(e)}", 500)

# Confirm Supabase upload endpoint
@app.route('/confirm-upload', methods=['POST'])
@app.route('/api/confirm-upload', methods=['POST'])
def confirm_supabase_upload():
    """Confirm that a file was successfully uploaded to Supabase"""
    try:
        data = request.get_json()
        if not data:
            return handle_error("Request body required", 400)
        
        file_path = data.get('file_path')
        original_filename = data.get('original_filename')
        file_size = data.get('file_size', 0)
        upload_token = data.get('upload_token')
        
        if not all([file_path, original_filename, upload_token]):
            return handle_error("file_path, original_filename, and upload_token are required", 400)
        
        # Since frontend handles direct upload, we trust the confirmation
        # Generate document ID for tracking
        document_id = str(uuid.uuid4())
        
        # Log the upload confirmation
        logger.info(f"Confirmed Supabase upload: {file_path} ({file_size} bytes)")
        
        return jsonify({
            'success': True,
            'message': 'File upload confirmed successfully! ðŸŽ‰',
            'document_id': document_id,
            'file_path': file_path,
            'original_filename': original_filename,
            'file_size': file_size,
            'storage_location': 'supabase',
            'upload_method': 'direct_supabase',
            'timestamp': datetime.utcnow().isoformat(),
            'status': 'Large file upload via Supabase confirmed',
            'next_steps': {
                'ai_processing': 'Ready for AI processing',
                'status_check': f'/api/status/{document_id}',
                'process': f'/api/process/{document_id}'
            }
        })
    
    except Exception as e:
        logger.error(f"Upload confirmation failed: {str(e)}")
        return handle_error(f"Upload confirmation failed: {str(e)}", 500)

# Upload chunk endpoint for fallback
@app.route('/upload-chunk', methods=['POST'])
@app.route('/api/upload-chunk', methods=['POST'])
def upload_chunk():
    """Handle chunked file upload (fallback method)"""
    return handle_error("Chunked upload not implemented. Please use direct Supabase upload for files >4MB.", 501)

# Upload endpoint
@app.route('/upload', methods=['POST', 'OPTIONS'])
@app.route('/api/upload', methods=['POST', 'OPTIONS'])
def upload_file():
    """Handle file upload with 25MB support"""
    if request.method == 'OPTIONS':
        return '', 200
    
    try:
        logger.info(f"Upload request - Content-Length: {request.headers.get('Content-Length', 'unknown')}")
        
        if 'file' not in request.files:
            return handle_error("No file provided", 400)
        
        file = request.files['file']
        if file.filename == '':
            return handle_error("No file selected", 400)
        
        if not allowed_file(file.filename):
            return handle_error("File type not allowed", 400)
        
        # Get file size
        file.seek(0, 2)
        file_size = file.tell()
        file.seek(0)
        
        logger.info(f"File: {file.filename}, Size: {file_size} bytes ({file_size / (1024*1024):.2f} MB)")
        
        # Validate file size
        if file_size > MAX_CONTENT_LENGTH:
            max_mb = MAX_CONTENT_LENGTH // (1024*1024)
            return handle_error(f"File too large. Maximum size: {max_mb}MB", 413)
        
        # Save file temporarily
        original_filename = secure_filename(file.filename)
        import uuid
        file_id = str(uuid.uuid4())
        temp_path = os.path.join(UPLOAD_FOLDER, f"{file_id}_{original_filename}")
        
        file.save(temp_path)
        
        # Verify file was saved
        actual_size = os.path.getsize(temp_path) if os.path.exists(temp_path) else 0
        logger.info(f"File saved - Expected: {file_size}, Actual: {actual_size}")
        
        return jsonify({
            'success': True,
            'message': 'File uploaded successfully! ðŸŽ‰',
            'document_id': file_id,
            'original_filename': original_filename,
            'file_size': file_size,
            'actual_size': actual_size,
            'upload_timestamp': datetime.utcnow().isoformat(),
            'status': '25MB upload capability confirmed',
            'next_steps': {
                'ai_processing': 'Would be initiated here in full version',
                'storage': 'Would be uploaded to Supabase storage',
                'status_check': f'/api/status/{file_id}'
            }
        })
        
    except RequestEntityTooLarge:
        max_mb = MAX_CONTENT_LENGTH // (1024*1024)
        return handle_error(f"File too large. Maximum size: {max_mb}MB", 413)
    except Exception as e:
        logger.error(f"Upload failed: {str(e)}")
        return handle_error(f"Upload failed: {str(e)}", 500)

# Supabase webhook endpoint
@app.route('/webhook/supabase', methods=['POST'])
@app.route('/api/webhook/supabase', methods=['POST'])
def supabase_webhook():
    """Handle Supabase storage webhooks for processing triggers"""
    try:
        # Verify webhook signature if needed
        # webhook_secret = os.environ.get('SUPABASE_WEBHOOK_SECRET')
        
        data = request.get_json()
        if not data:
            return handle_error("Webhook payload required", 400)
        
        event_type = data.get('type')
        record = data.get('record', {})
        
        logger.info(f"Received Supabase webhook: {event_type}")
        
        if event_type == 'INSERT' and record.get('bucket_id') == SUPABASE_BUCKET:
            # File was uploaded to our documents bucket
            file_path = record.get('name')
            file_size = record.get('metadata', {}).get('size', 0)
            
            if file_path and file_size > 0:
                # Generate document ID and trigger processing
                document_id = str(uuid.uuid4())
                
                # TODO: Store document metadata and queue for AI processing
                logger.info(f"Queuing document {document_id} for processing: {file_path}")
                
                return jsonify({
                    'success': True,
                    'message': 'Webhook processed successfully',
                    'document_id': document_id,
                    'file_path': file_path,
                    'file_size': file_size,
                    'action': 'queued_for_processing',
                    'timestamp': datetime.utcnow().isoformat()
                })
        
        # For other events, just acknowledge
        return jsonify({
            'success': True,
            'message': 'Webhook received',
            'event_type': event_type,
            'timestamp': datetime.utcnow().isoformat()
        })
    
    except Exception as e:
        logger.error(f"Webhook processing failed: {str(e)}")
        return handle_error(f"Webhook processing failed: {str(e)}", 500)

# Process document endpoint
@app.route('/process/<document_id>', methods=['POST'])
@app.route('/api/process/<document_id>', methods=['POST'])
def process_document(document_id: str):
    """Process a document by ID (works with both server and Supabase storage)"""
    try:
        data = request.get_json() or {}
        storage_location = data.get('storage_location', 'server')
        file_path = data.get('file_path')
        
        if not file_path:
            return handle_error("file_path is required", 400)
        
        logger.info(f"Processing document {document_id} from {storage_location}: {file_path}")
        
        # TODO: Implement actual AI processing
        # For now, simulate processing
        processing_result = {
            'document_id': document_id,
            'file_path': file_path,
            'storage_location': storage_location,
            'processing_status': 'completed',
            'ai_analysis': {
                'document_type': 'real_estate',
                'confidence': 0.95,
                'extracted_data': {
                    'property_address': 'Example property analysis',
                    'transaction_amount': 'Would be extracted by AI'
                }
            },
            'processing_time': '2.3 seconds',
            'timestamp': datetime.utcnow().isoformat()
        }
        
        return jsonify({
            'success': True,
            'message': 'Document processed successfully',
            'results': processing_result
        })
    
    except Exception as e:
        logger.error(f"Document processing failed: {str(e)}")
        return handle_error(f"Document processing failed: {str(e)}", 500)

# Get signed URL for viewing files
@app.route('/file/<path:file_path>', methods=['GET'])
@app.route('/api/file/<path:file_path>', methods=['GET'])
def get_file_url(file_path: str):
    """Generate signed URL for viewing Supabase stored files"""
    try:
        if not supabase_client:
            return handle_error("File viewing not available - Supabase not configured", 503)
        
        # Create signed URL for file viewing (expires in 1 hour)
        result = supabase_client.storage.from_(SUPABASE_BUCKET).create_signed_url(
            file_path, expires_in=3600
        )
        
        if result and 'signedUrl' in result:
            return jsonify({
                'success': True,
                'signed_url': result['signedUrl'],
                'file_path': file_path,
                'expires_in': 3600,
                'timestamp': datetime.utcnow().isoformat()
            })
        else:
            return handle_error("Failed to create signed URL", 500)
    
    except Exception as e:
        logger.error(f"Failed to create signed URL for {file_path}: {str(e)}")
        return handle_error(f"Failed to get file URL: {str(e)}", 500)

# Status check endpoint
@app.route('/status/<document_id>', methods=['GET'])
@app.route('/api/status/<document_id>', methods=['GET'])
def get_status(document_id: str):
    """Get processing status"""
    return jsonify({
        'success': True,
        'document_id': document_id,
        'status': 'uploaded',
        'message': 'Large file upload successful via new architecture - Ready for AI processing',
        'timestamp': datetime.utcnow().isoformat(),
        'progress': 100,
        'stage': 'upload_complete',
        'next_actions': {
            'process': f'/api/process/{document_id}',
            'download_results': f'/api/results/{document_id}'
        }
    })

# Root endpoint
@app.route('/', methods=['GET'])
@app.route('/api/', methods=['GET'])
@app.route('/api', methods=['GET'])
def root():
    """API root endpoint"""
    upload_methods = ['server_upload']
    if supabase_client:
        upload_methods.append('direct_supabase')
    
    return jsonify({
        'message': 'RExeli API - Enhanced Upload Architecture! ðŸš€',
        'version': '3.0.0',
        'status': 'operational',
        'capabilities': {
            'max_file_size': f"{MAX_FILE_SIZE // (1024*1024)}MB",
            'direct_upload_threshold': f"{DIRECT_UPLOAD_THRESHOLD // (1024*1024)}MB",
            'supported_formats': list(ALLOWED_EXTENSIONS),
            'upload_methods': upload_methods,
            'features': [
                'Intelligent upload routing',
                '50MB+ file support via Supabase',
                'Direct browser-to-storage uploads',
                'Bypass Vercel 4.5MB limitation',
                'File validation and security'
            ]
        },
        'workflow': {
            'small_files': 'Browser â†’ Vercel Function (â‰¤4MB)',
            'large_files': 'Browser â†’ Supabase Storage (â‰¤50MB)',
            'processing': 'Webhook â†’ Vercel Function â†’ AI Processing'
        },
        'endpoints': {
            'health': '/api/health',
            'upload_method': '/api/upload-method (POST)',
            'upload_small': '/api/upload (POST)',
            'confirm_large': '/api/confirm-upload (POST)',
            'status': '/api/status/{document_id}',
            'ai_classify': '/api/classify-document (POST)',
            'ai_regions': '/api/suggest-regions (POST)',
            'ai_extract': '/api/extract-data (POST)',
            'ai_validate': '/api/validate-data (POST)',
            'ai_quality': '/api/quality-score (POST)'
        }
    })


# AI Processing Endpoints

def get_file_content(filepath: str) -> str:
    """Download file content from Supabase storage"""
    if not supabase_client:
        raise Exception("Supabase client not available")
    
    try:
        # Generate signed URL for the file
        response = supabase_client.storage.from_(SUPABASE_BUCKET).create_signed_url(filepath, 3600)
        if not response.get('signedURL'):
            raise Exception("Failed to create signed URL")
        
        return response['signedURL']
    except Exception as e:
        logger.error(f"Failed to get file content for {filepath}: {str(e)}")
        raise


@app.route('/api/classify-document', methods=['POST'])
def classify_document():
    """Classify document type using AI"""
    try:
        data = request.get_json()
        filepath = data.get('filepath')
        
        if not filepath:
            return handle_error("File path is required", 400)
        
        if not openai_client:
            return handle_error("AI service not available", 503)
        
        # Get file URL
        file_url = get_file_content(filepath)
        
        # Use OpenAI vision to classify document
        response = openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": """You are a commercial real estate expert analyzing documents. Classify this real estate document type with high accuracy.

DOCUMENT TYPES TO IDENTIFY:
- rent_roll: Contains tenant listings, rental rates, lease terms, occupancy data
- offering_memo: Marketing materials with property details, financial projections, investment highlights
- lease_agreement: Legal contracts between landlords and tenants with lease terms
- comparable_sales: Property sales comparisons with prices, cap rates, market data
- financial_statement: Income statements, cash flow, NOI, expenses, financial performance
- other: Any document that doesn't fit the above categories

Look for these KEY INDICATORS:
â€¢ Rent Roll: "Tenant", "Monthly Rent", "Lease Exp", "SF", "PSF", "%Occupied"
â€¢ Offering Memo: "Investment Opportunity", "Cap Rate", "NOI", "Offering Price"
â€¢ Lease Agreement: "Lease Agreement", "Landlord", "Tenant", "Term", "Monthly Rent"
â€¢ Comparable Sales: "Sales Price", "Price/SF", "Cap Rate", "Comparable Properties"
â€¢ Financial Statement: "Income", "Expenses", "NOI", "Cash Flow", "P&L"

Respond with JSON only:
{
  "document_type": "rent_roll|offering_memo|lease_agreement|comparable_sales|financial_statement|other",
  "confidence": 0.95,
  "reasoning": "Detailed explanation based on specific content identified"
}"""
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": file_url
                            }
                        }
                    ]
                }
            ],
            max_tokens=500,
            temperature=0.1
        )
        
        result_text = response.choices[0].message.content.strip()
        classification = json.loads(result_text)
        
        return jsonify({
            'success': True,
            'data': {
                'classification': classification
            }
        })
        
    except Exception as e:
        logger.error(f"Document classification failed: {str(e)}")
        return handle_error(f"Classification failed: {str(e)}", 500)


@app.route('/api/suggest-regions', methods=['POST'])
def suggest_regions():
    """Suggest data regions in document using AI"""
    try:
        data = request.get_json()
        filepath = data.get('filepath')
        document_type = data.get('documentType', 'unknown')
        
        if not filepath:
            return handle_error("File path is required", 400)
        
        if not openai_client:
            return handle_error("AI service not available", 503)
        
        # Get file URL
        file_url = get_file_content(filepath)
        
        # Use OpenAI vision to identify data regions
        response = openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": f"""You are a commercial real estate expert analyzing a {document_type} document. Identify ALL important data regions for Excel extraction.

CRITICAL REAL ESTATE DATA TO FIND:

FOR RENT ROLLS:
- Property address and basic info
- Tenant names and company information
- Unit/suite numbers and square footage
- Monthly rent amounts and rent per SF
- Lease start/end dates and terms
- Security deposits and additional charges
- Occupancy status and vacancy data

FOR OFFERING MEMOS:
- Property address and description
- Total rentable square footage
- Cap rate and NOI figures
- Purchase price and pricing metrics
- Tenant information and major tenants
- Financial highlights and projections

FOR LEASE AGREEMENTS:
- Property address and unit details
- Landlord and tenant information
- Lease term dates and rent amounts
- Security deposit and fees
- Square footage and rent per SF
- Special clauses and conditions

FOR COMPARABLE SALES:
- Property addresses and details
- Sale prices and price per SF
- Cap rates and NOI data
- Square footage and lot size
- Sale dates and market conditions

FOR FINANCIAL STATEMENTS:
- Rental income line items
- Operating expense categories
- Net operating income (NOI)
- Cash flow and return metrics
- Property taxes and insurance

Provide EXACT coordinates as percentages of document dimensions. Look for tables, lists, and key-value pairs.

Respond with JSON only:
{{
  "regions": [
    {{
      "id": "unique_id",
      "label": "Descriptive label (e.g., Tenant Rent Table, Property Address, NOI)",
      "type": "table|text|number|date",
      "x": 10,
      "y": 20,
      "width": 200,
      "height": 30,
      "confidence": 0.9,
      "page": 0,
      "priority": "high|medium|low"
    }}
  ]
}}"""
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": file_url
                            }
                        }
                    ]
                }
            ],
            max_tokens=1000,
            temperature=0.1
        )
        
        result_text = response.choices[0].message.content.strip()
        regions_data = json.loads(result_text)
        
        return jsonify({
            'success': True,
            'data': regions_data
        })
        
    except Exception as e:
        logger.error(f"Region suggestion failed: {str(e)}")
        return handle_error(f"Region suggestion failed: {str(e)}", 500)


@app.route('/api/extract-data', methods=['POST'])
def extract_data():
    """Extract data from specified regions using AI"""
    try:
        data = request.get_json()
        filepath = data.get('filepath')
        regions = data.get('regions', [])
        
        if not filepath:
            return handle_error("File path is required", 400)
        
        if not openai_client:
            return handle_error("AI service not available", 503)
        
        # Get file URL
        file_url = get_file_content(filepath)
        
        # Create region descriptions for AI
        region_descriptions = []
        for region in regions:
            region_descriptions.append(f"- {region.get('label', 'Unknown')} at position ({region.get('x', 0)}, {region.get('y', 0)})")
        
        regions_text = "\n".join(region_descriptions)
        
        # Use OpenAI vision to extract data from regions
        response = openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": f"""You are a commercial real estate data extraction specialist. Extract precise data from these identified regions for Excel export:

REGIONS TO EXTRACT:
{regions_text}

EXTRACTION GUIDELINES:
â€¢ For CURRENCY: Extract as numbers only (e.g., "1500.00" not "$1,500")
â€¢ For DATES: Use MM/DD/YYYY format
â€¢ For PERCENTAGES: Extract as decimal (e.g., "0.065" for 6.5%)
â€¢ For SQUARE FOOTAGE: Extract numbers only
â€¢ For ADDRESSES: Include full address with suite/unit numbers
â€¢ For TENANT NAMES: Extract company names exactly as written
â€¢ For TABLES: Extract each cell's value separately if the region contains multiple data points

CRITICAL: Each extracted value must be Excel-ready (clean numbers, consistent formatting).

Return JSON with extracted values:
{{
  "extracted_data": {{
    "region_id_1": {{
      "value": "clean extracted value",
      "raw_text": "original text if different from value",
      "confidence": 0.95,
      "type": "currency|percentage|date|address|name|number|text",
      "excel_column": "suggested column name for Excel"
    }}
  }}
}}"""
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": file_url
                            }
                        }
                    ]
                }
            ],
            max_tokens=1500,
            temperature=0.1
        )
        
        result_text = response.choices[0].message.content.strip()
        extraction_data = json.loads(result_text)
        
        return jsonify({
            'success': True,
            'data': extraction_data
        })
        
    except Exception as e:
        logger.error(f"Data extraction failed: {str(e)}")
        return handle_error(f"Data extraction failed: {str(e)}", 500)


@app.route('/api/validate-data', methods=['POST'])
def validate_data():
    """Validate extracted data using AI"""
    try:
        data = request.get_json()
        extracted_data = data.get('extractedData', {})
        document_type = data.get('documentType', 'unknown')
        
        if not extracted_data:
            return handle_error("Extracted data is required", 400)
        
        if not openai_client:
            return handle_error("AI service not available", 503)
        
        # Use OpenAI to validate data
        response = openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": f"""You are a commercial real estate data validation expert. Validate this extracted {document_type} data for accuracy and completeness:

{json.dumps(extracted_data, indent=2)}

VALIDATION CHECKS FOR COMMERCIAL REAL ESTATE:

RENT ROLL VALIDATION:
â€¢ Monthly rent must be positive numbers
â€¢ Rent per SF should be reasonable ($15-150/SF annually)
â€¢ Lease dates must be valid and logical (start < end)
â€¢ Square footage must be positive
â€¢ Occupancy rates between 0-100%

OFFERING MEMO VALIDATION:
â€¢ Cap rates typically 3-12%
â€¢ NOI must be positive for income properties
â€¢ Price per SF should be market reasonable
â€¢ Financial projections should be logical

LEASE AGREEMENT VALIDATION:
â€¢ Security deposits typically 1-3 months rent
â€¢ Lease terms should be standard (1, 3, 5, 10 years)
â€¢ Rent escalations should be reasonable (2-4% annually)

COMPARABLE SALES VALIDATION:
â€¢ Sale prices must be positive
â€¢ Cap rates should be within market range
â€¢ Price per SF should align with market data

GENERAL VALIDATIONS:
â€¢ Currency values properly formatted
â€¢ Dates in valid format
â€¢ Addresses complete and properly formatted
â€¢ No negative values where impossible
â€¢ Percentages in decimal format (0-1)

Respond with JSON only:
{{
  "validation": {{
    "is_valid": true,
    "errors": ["Specific error descriptions"],
    "warnings": ["Data quality concerns"],
    "suggestions": ["Improvements for Excel export"],
    "confidence": 0.9,
    "data_quality_score": 0.85
  }}
}}"""
                }
            ],
            max_tokens=500,
            temperature=0.1
        )
        
        result_text = response.choices[0].message.content.strip()
        validation_data = json.loads(result_text)
        
        return jsonify({
            'success': True,
            'data': validation_data
        })
        
    except Exception as e:
        logger.error(f"Data validation failed: {str(e)}")
        return handle_error(f"Data validation failed: {str(e)}", 500)


@app.route('/api/quality-score', methods=['POST'])
def calculate_quality_score():
    """Calculate quality score for extracted data"""
    try:
        data = request.get_json()
        extracted_data = data.get('extractedData', {})
        validation_results = data.get('validationResults', {})
        
        if not extracted_data:
            return handle_error("Extracted data is required", 400)
        
        # Simple quality scoring algorithm
        total_fields = len(extracted_data)
        valid_fields = 0
        confidence_sum = 0
        
        for field_data in extracted_data.values():
            if isinstance(field_data, dict):
                confidence = field_data.get('confidence', 0)
                confidence_sum += confidence
                if confidence > 0.7:  # Threshold for "valid"
                    valid_fields += 1
        
        completeness_score = valid_fields / total_fields if total_fields > 0 else 0
        avg_confidence = confidence_sum / total_fields if total_fields > 0 else 0
        
        # Factor in validation results
        validation_score = 1.0
        validation = validation_results.get('validation', {})
        if validation:
            if not validation.get('is_valid', True):
                validation_score -= 0.3
            
            errors = len(validation.get('errors', []))
            warnings = len(validation.get('warnings', []))
            validation_score -= (errors * 0.1) + (warnings * 0.05)
            validation_score = max(0, validation_score)
        
        overall_score = (completeness_score + avg_confidence + validation_score) / 3
        
        return jsonify({
            'success': True,
            'data': {
                'quality_score': {
                    'overall': overall_score,
                    'completeness': completeness_score,
                    'confidence': avg_confidence,
                    'validation': validation_score,
                    'total_fields': total_fields,
                    'valid_fields': valid_fields
                }
            }
        })
        
    except Exception as e:
        logger.error(f"Quality score calculation failed: {str(e)}")
        return handle_error(f"Quality score calculation failed: {str(e)}", 500)


# Error handlers
@app.errorhandler(413)
def handle_file_too_large(error):
    max_mb = MAX_CONTENT_LENGTH // (1024*1024)
    return handle_error(f"File too large. Maximum size: {max_mb}MB", 413)

@app.errorhandler(404)
def handle_not_found(error):
    return handle_error("API endpoint not found", 404)

@app.errorhandler(500)
def handle_internal_error(error):
    return handle_error("Internal server error", 500)

if __name__ == '__main__':
    app.run(debug=True, port=5000)